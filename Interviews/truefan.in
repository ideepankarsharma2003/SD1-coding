# ğŸš€ Open Questions & Learnings

## ğŸ“ Remarks from TrueFan.com Interview
Reflect on:
- Positive aspects of the interview.
- Good questions asked by the interviewer.
- How their guidance helped when stuck on basic questions.
- Key **learning experiences** from the process.

## ğŸ” Tanh vs. Sigmoid Activation Functions
- Why do they exist separately, despite **tanh being a linear transformation of sigmoid**?
- Differences in output range (**0-1 for sigmoid, -1 to 1 for tanh**) and impact on **training/convergence**.
- Importance of their **derivatives**â€”tanhâ€™s steeper derivative around zero **reduces vanishing gradients**.

## ğŸ”¬ Batch Normalization in Generative Models (GANs/VAEs)
- Why is **batch normalization generally avoided** or used differently in **GANs/VAEs**?
- Considerations in generative settings, such as **mode collapse and sample diversity**.

## ğŸ§  Perceptron & XOR Problem
- Why **canâ€™t a single-layer perceptron** solve the **XOR problem**?
- Discuss **linear separability** limitations and how **multi-layer networks resolve** it.

---

### ğŸ¯ Next Steps
- [ ] Research explanations for each topic.
- [ ] Identify relevant papers or sources.
- [ ] Discuss findings in team reviews or personal study sessions.

ğŸ“Œ **Tagging:** `deep-learning`, `ml-theory`, `study-notes`
