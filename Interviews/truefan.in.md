# 🚀 Open Questions & Learnings

## 📝 Remarks from TrueFan.com Interview
Reflect on:
- Positive aspects of the interview.
- Good questions asked by the interviewer.
- How their guidance helped when stuck on basic questions.
- Key **learning experiences** from the process.

## 🔎 Tanh vs. Sigmoid Activation Functions
- Why do they exist separately, despite **tanh being a linear transformation of sigmoid**?
- Differences in output range (**0-1 for sigmoid, -1 to 1 for tanh**) and impact on **training/convergence**.
- Importance of their **derivatives**—tanh’s steeper derivative around zero **reduces vanishing gradients**.

## 🔬 Batch Normalization in Generative Models (GANs/VAEs)
- Why is **batch normalization generally avoided** or used differently in **GANs/VAEs**?
- Considerations in generative settings, such as **mode collapse and sample diversity**.

## 🧠 Perceptron & XOR Problem
- Why **can’t a single-layer perceptron** solve the **XOR problem**?
- Discuss **linear separability** limitations and how **multi-layer networks resolve** it.

---

### 🎯 Next Steps
- [ ] Research explanations for each topic.
- [ ] Identify relevant papers or sources.
- [ ] Discuss findings in team reviews or personal study sessions.

📌 **Tagging:** `deep-learning`, `ml-theory`, `study-notes`
